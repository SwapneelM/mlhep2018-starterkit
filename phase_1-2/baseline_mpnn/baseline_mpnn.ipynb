{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from random import shuffle\n",
    "from IPython.display import clear_output\n",
    "from sklearn import metrics\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "USE_GPU = True\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1' if USE_GPU else ''\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phase 1 set num_classes=3\n",
    "# for phase 2 set num_classes=4\n",
    "num_classes = 4\n",
    "energy_cut = 0.01\n",
    "\n",
    "zero_class_exists = False\n",
    "\n",
    "num_nn_output = num_classes - (not zero_class_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'path/to/train_3-4.hdf5'\n",
    "test_file = 'path/to/test_3_4.hdf5'\n",
    "submission_file = 'submission_3-4.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from tools.base import plot_3d, hdf5_to_numpy, plot_3d_with_edges\n",
    "from tools.tools import stretch_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Block 1\n",
    "# You can read as many training examples as you like\n",
    "# Currently I haven't trained on the entire dataset available to us \n",
    "\n",
    "%%time\n",
    "# N -- number of enties to read. Either int or np.inf. In latter case all entries are readed.\n",
    "N = 2500\n",
    "X, Y, M, N = hdf5_to_numpy(file=train_file, n=N, num_classes=num_classes, \n",
    "                           zero_class_exists=zero_class_exists, energy_cut=energy_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "plot_3d(X[k], Y[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of graph dataset\n",
    "\n",
    "\n",
    "![title](img/knn_graph.png)\n",
    "\n",
    "To compute graph on which we are going to do inference we will use K-nearest neighbours graph. This algorithm draws edges from node(which is, in our case, a hit in the detector) to K closest points.\n",
    "\n",
    "Tunable parameter:\n",
    "\n",
    "__n_neighbors__ -- number of neighbours for k-nearest neighbours graph algo(http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html).\n",
    "\n",
    "### Ideas\n",
    "\n",
    "  * try to play with default params;\n",
    "  * explore different ideas for graph computation: heuristics, radius neighbors graph, etc.;\n",
    "  * different metrics: manhattan, l1, cosine, metric learning(__hot!__), etc.;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 20\n",
    "\n",
    "in_degree_max, out_degree_max = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.simplified_clustering import generate_graph_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_clusters_graph = []\n",
    "for k in tqdm(range(len(X))):\n",
    "    if len(X[k]) == 0:\n",
    "        continue\n",
    "    # construction of graph based on aggregated statistics\n",
    "    X_cluster_graph, in_degree_max_local, out_degree_max_local = generate_graph_dataset(X=X[k], Y=Y[k], M=M[k],\n",
    "                                                                                        n_neighbors=n_neighbors)\n",
    "    in_degree_max = max(in_degree_max_local, in_degree_max)\n",
    "    out_degree_max = max(out_degree_max_local, out_degree_max)\n",
    "    \n",
    "    X_clusters_graph.append(X_cluster_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is __in_degree_max__ and __out_degree_max__?\n",
    "\n",
    "Well, when you are working with tensorflow you have to specify shape of your data(at least number of columns).\n",
    "\n",
    "```\n",
    "shape = (number_of_nodes, out_degree/in_degree)\n",
    "```\n",
    "\n",
    "__max_out_degree__ is fixed and equal __n_neighbors__ in our setting, but __in_degree_max__ could be different across different events. \n",
    "\n",
    "To anticipate it we are padding all events with edges to non-existing node. Latter this should be taken into account in the MPNN-algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_max, out_degree_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "for X_cluster_graph in X_clusters_graph:\n",
    "    X_cluster_graph['X_cluster_messages_out'] = stretch_array(X_cluster_graph['X_cluster_messages_out'], \n",
    "                                                              n=out_degree_max, \n",
    "                                                              fill_value=len(X_cluster_graph['X_cluster_edges']))\n",
    "    \n",
    "    X_cluster_graph['X_cluster_messages_in'] = stretch_array(X_cluster_graph['X_cluster_messages_in'], \n",
    "                                                              n=in_degree_max, \n",
    "                                                              fill_value=len(X_cluster_graph['X_cluster_edges']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model (MPNN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, GRUCell, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_graph['X_cluster_nodes'].shape, X_cluster_graph['X_cluster_edges'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__X_nodes__ -- features per hit(i.e. energy).\n",
    "\n",
    "__X_edges__ -- features for each edge that connects two hits(i.e. relative difference of coordinates).\n",
    "\n",
    "__X_labels__ -- labels ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim_features_nodes = 4\n",
    "ndim_features_edges = 5\n",
    "ndim_message = 6\n",
    "\n",
    "X_nodes = K.placeholder(shape=(None, ndim_features_nodes)) # features of nodes\n",
    "X_edges = K.placeholder(shape=(None, ndim_features_edges)) # features of edges\n",
    "X_labels = K.placeholder(shape=(None, num_nn_output)) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__X_nodes_in_out__ -- edge list.\n",
    "\n",
    "__X_messages_in__ -- in-adjacency lists.\n",
    "\n",
    "__X_messages_out__ -- out-adjacency lists.\n",
    "\n",
    "All these graph representations are equivalent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nodes_in_out = K.placeholder(shape=(None, 2), dtype=np.int32, name=\"nodes_in_out\") # edges\n",
    "X_messages_in = K.placeholder(shape=(None, in_degree_max), dtype=np.int32, name=\"messages_in\") # shape = (none, size of neighbourhood)\n",
    "X_messages_out = K.placeholder(shape=(None, out_degree_max), dtype=np.int32, name=\"messages_out\") # shape = (none, size of neighbourhood)\n",
    "\n",
    "# fake messages to(or from) non-existing node\n",
    "fake_message_const = K.constant(value=[ndim_message * [-np.inf]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholders = {\n",
    "    'X_nodes': X_nodes,\n",
    "    'X_edges': X_edges,\n",
    "    'X_labels': X_labels,\n",
    "    'X_nodes_in_out': X_nodes_in_out,\n",
    "    'X_messages_in': X_messages_in,\n",
    "    'X_messages_out': X_messages_out\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 3\n",
    "\n",
    "message_passers = {\n",
    "    0: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.05),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.05),\n",
    "                  ]\n",
    "                 ),\n",
    "    1: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.05),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.05),\n",
    "                  ]\n",
    "                 ),    \n",
    "    2: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.05),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.05),\n",
    "                  ]\n",
    "                 )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_updater = tf.contrib.rnn.GRUCell(num_units=ndim_features_nodes, )\n",
    "state_updater = Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_message + ndim_features_nodes,), activation=relu), \n",
    "                      Dense(ndim_features_nodes),\n",
    "                                  ], name=\"state_updater\"\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout = Dense(num_nn_output, input_shape=(ndim_features_nodes,), activation=keras.activations.softmax, name=\"readout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPNN construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of MPNN algorithm in a diagram for a following toy graph:\n",
    "\n",
    "![](img/example_graph.png)\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "![](img/mpnn.png)\n",
    "\n",
    "\n",
    "And corresponding code with comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(X_nodes, X_edges, X_nodes_in_out, \n",
    "                  X_messages_in, X_messages_out, message_passers, \n",
    "                  state_updater, readout, ndim_features_nodes, fake_message_const, steps):\n",
    "    # nodes 'talks' to each other several times which is defined by __step__ parameter\n",
    "    for step in range(steps):\n",
    "        # messages from node to node\n",
    "        messages = message_passers[step](\n",
    "            K.concatenate(\n",
    "                [\n",
    "                    K.reshape(K.gather(reference=X_nodes, indices=X_nodes_in_out), \n",
    "                              shape=(-1, 2 * ndim_features_nodes)), \n",
    "                    X_edges\n",
    "                ], axis=1\n",
    "            )\n",
    "        )\n",
    "        # correct dealing with non-existing edge\n",
    "        messages = K.concatenate([messages, fake_message_const], axis=0)\n",
    "        messages = tf.where(tf.is_inf(messages), tf.zeros_like(messages), messages)\n",
    "\n",
    "        # aggregating messages that came into the node\n",
    "        messages_aggregated_in = K.max(K.gather(reference=messages, indices=X_messages_in), axis=1)\n",
    "        # ... and those exiting node\n",
    "        messages_aggregated_out = K.max(K.gather(reference=messages, indices=X_messages_out), axis=1)\n",
    "\n",
    "        # update nodes states based on messages and previous state\n",
    "        X_nodes = state_updater(K.concatenate([messages_aggregated_in, messages_aggregated_out, X_nodes], axis=1))\n",
    "\n",
    "    return readout(X_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.mpnn import build_network, run_train, run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predictions = build_network(X_nodes=X_nodes, \n",
    "                              X_edges=X_edges, \n",
    "                              X_nodes_in_out=X_nodes_in_out, \n",
    "                              X_messages_in=X_messages_in, \n",
    "                              X_messages_out=X_messages_out, \n",
    "                              message_passers=message_passers, \n",
    "                              state_updater=state_updater, \n",
    "                              readout=readout, \n",
    "                              steps=steps, \n",
    "                              ndim_features_nodes=ndim_features_nodes,\n",
    "                              fake_message_const=fake_message_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tf = tf.reduce_mean(keras.losses.categorical_crossentropy(X_labels, X_predictions))\n",
    "accuracy_tf = tf.reduce_mean(keras.metrics.categorical_accuracy(X_labels, X_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=2e-3).minimize(loss_tf, var_list=tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "init.run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(len(X_clusters_graph) * 0.8)\n",
    "print(TRAIN_SIZE)\n",
    "shuffle(X_clusters_graph)\n",
    "\n",
    "X_clusters_graph_train = X_clusters_graph[:TRAIN_SIZE]\n",
    "X_clusters_graph_eval = X_clusters_graph[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter(\n",
    "    os.path.join(\"/mnt/students-home/test-student/tensorflow-logs\", \"MPNN_v5\", \"train\"), sess.graph\n",
    ")\n",
    "merged_summary = tf.summary.merge([\n",
    "    tf.summary.scalar(\"loss\", loss_tf),\n",
    "    tf.summary.scalar(\"accuracy\", accuracy_tf)\n",
    "])\n",
    "# train_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "roc_aucs = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(500)):\n",
    "    loss_float = 0\n",
    "    accuracy_float = 0\n",
    "    \n",
    "    losses_epoch = []\n",
    "    accuracies_epoch = []\n",
    "    roc_aucs_epoch = []\n",
    "    for X_cluster_graph in X_clusters_graph_train:\n",
    "        predictions, (loss, accuracy, summary) = run_train(X_cluster_graph=X_cluster_graph,\n",
    "                                   X_predictions=X_predictions,\n",
    "                                   optimizer=optimizer, sess=sess, \n",
    "                                   ndim_features_nodes=ndim_features_nodes, \n",
    "                                   ndim_features_edges=ndim_features_edges, \n",
    "                                   placeholders=placeholders,\n",
    "                                   metrics=[loss_tf, accuracy_tf, merged_summary])\n",
    "        losses_epoch.append(loss)\n",
    "        accuracies_epoch.append(accuracy)\n",
    "    clear_output()\n",
    "    \n",
    "    losses.append(np.mean(losses_epoch))\n",
    "    accuracies.append(np.mean(accuracies_epoch))\n",
    "    train_writer.add_summary(summary, epoch)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        clear_output()\n",
    "        \n",
    "        plt.figure(figsize=(10,4))\n",
    "        ax = plt.subplot(121)\n",
    "        plt.title('log-loss')\n",
    "        plt.plot(losses)\n",
    "\n",
    "        ax = plt.subplot(122)\n",
    "        plt.title('accuracy')\n",
    "        plt.plot(accuracies)\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metadata = tf.RunMetadata()\n",
    "train_writer.add_run_metadata(run_metadata, 'epoch_%d' % epoch)\n",
    "train_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test = []\n",
    "accuracies_test = []\n",
    "roc_aucs_test = []\n",
    "predictions_total = [] \n",
    "y_total =[]\n",
    "\n",
    "for X_cluster_graph in X_clusters_graph_eval:\n",
    "    predictions = run_test(X_cluster_graph=X_cluster_graph, \n",
    "                                              X_predictions=X_predictions,\n",
    "                                              sess=sess, \n",
    "                                              ndim_features_nodes=ndim_features_nodes, \n",
    "                                              ndim_features_edges=ndim_features_edges, \n",
    "                                              placeholders=placeholders)\n",
    "    X_cluster_graph['predictions'] = predictions\n",
    "    predictions_total.append(predictions)\n",
    "    y_total.append(X_cluster_graph['Y_cluster_labels'])\n",
    "    losses_test.append(loss)\n",
    "    accuracies_test.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_total = np.concatenate(predictions_total)\n",
    "y_total = np.concatenate(y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(np.argmax(y_total, axis=1), np.argmax(predictions_total, axis=1))\n",
    "roc_auc = metrics.roc_auc_score(y_total, predictions_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Block 2\n",
    "\n",
    "# Run this section the first time but no need to keep re-running it since it does not undergo modification\n",
    "X_test, Y_test, M_test, N_test = hdf5_to_numpy(file=test_file, n=np.inf, num_classes=num_classes, test=True,\n",
    "                                               zero_class_exists=zero_class_exists, energy_cut=energy_cut)\n",
    "\n",
    "# This saves the data to a file so you only need to do this once\n",
    "# If you need to run the above section again, instead consider loading this file since it is much faster\n",
    "np.savez_compressed('saved_test_data.npz', X_test=X_test, Y_test=Y_test, M_test=M_test, N_test=N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Block 3\n",
    "\n",
    "# Loading your data back when you HAVE NOT RUN the above block\n",
    "# This is highly inefficient and consumes a ton of memory but it was a hack I built at the time\n",
    "# Try using the csv-compatible notebook in phase_3-4/baseline_mpnn/ instead\n",
    "test_data = np.load('saved_test_data.npz')\n",
    "X_test = test_data['X_test']\n",
    "Y_test = test_data['Y_test']\n",
    "M_test = test_data['M_test']\n",
    "N_test = test_data['N_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 0 if zero_class_exists else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "expectedrows = len(X_test)\n",
    "FILTERS = tables.Filters(complevel=5, complib='zlib', shuffle=True, bitshuffle=False, fletcher32=False, least_significant_digit=None)\n",
    "f_submission = tables.open_file(submission_file, 'w', filters=FILTERS)\n",
    "preds_array = f_submission.create_earray('/', 'pred', tables.UInt32Atom(), (0,192,192,192), expectedrows=expectedrows)\n",
    "\n",
    "for k in tqdm(range(expectedrows)):\n",
    "    submission_answer = np.zeros((192, 192, 192))\n",
    "    if len(X_test[k]) == 0:\n",
    "        pass\n",
    "    else: \n",
    "        X_cluster_graph, in_degree_max_local, out_degree_max_local = generate_graph_dataset(X=X_test[k], \n",
    "                                                                                            Y=Y_test[k], \n",
    "                                                                                            M=M_test[k],\n",
    "                                                                                            n_neighbors=n_neighbors, \n",
    "                                                                                            in_degree_max=in_degree_max, \n",
    "                                                                                            out_degree_max=out_degree_max)\n",
    "\n",
    "        predictions = run_test(X_cluster_graph=X_cluster_graph, \n",
    "                                                  X_predictions=X_predictions,\n",
    "                                                  sess=sess, \n",
    "                                                  ndim_features_nodes=ndim_features_nodes, \n",
    "                                                  ndim_features_edges=ndim_features_edges, \n",
    "                                                  placeholders=placeholders)\n",
    "        submission_answer[X_cluster_graph['M'][:, 0], X_cluster_graph['M'][:, 1], X_cluster_graph['M'][:, 2]] = np.argmax(predictions, axis=1) + shift\n",
    "        X_cluster_graph['submission_answer'] = submission_answer\n",
    "    \n",
    "    preds_array.append(np.expand_dims(submission_answer, axis=0))\n",
    "    del submission_answer\n",
    "preds_array.close()\n",
    "f_submission.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_zip = \"submission-%s.zip\" % (datetime.now().strftime(\"%Y-%m-%d-%H%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(submission_file_zip, 'w', compression=zipfile.ZIP_DEFLATED) as myzip:\n",
    "    myzip.write(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 test-student test-student 55421 Aug 10 02:06 submission-2018-08-10-0206.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lt *zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission-2018-08-10-0206.zip' target='_blank'>submission-2018-08-10-0206.zip</a><br>"
      ],
      "text/plain": [
       "/mnt/students-home/test-student/anaderi-s2/phase_1-2/baseline_mpnn/submission-2018-08-10-0206.zip"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(submission_file_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final tips & tricks\n",
    "\n",
    "If you want to drive up your score try following things:\n",
    "\n",
    "  * stack MOAR layer;\n",
    "  * more epochs;\n",
    "  * change Dense `state_updater` on LSTM/GRU `state_updater`(btw, you just need to uncomment a bit of code in the section `NNs` and in `tools.mpnn.build_network`);\n",
    "  * data augmentation;\n",
    "  * btw, you might have noticed that I use different `message_passers` but single `state_updater` for each step. This is called _weight tightening_ and used to deal with overfitting;\n",
    "  \n",
    "  You can apply the same technique to `message_passers` or, alternatively, unravel `state_updater`. It's all up to you!\n",
    "  \n",
    "  * do not discard domain knowledge. Even if it's not applicable to feature engeneering you still can use in graph construction / smart clustering / loss function choice / structure of MPNN.\n",
    "  * this version of MPNN learns on one sample per iteration, using batches could impove quality;\n",
    "  * play with learning rate / optimizer type;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
